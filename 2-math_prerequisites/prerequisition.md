
## 1. 주요 수학 표기법 (Math Notation)

수학적 아이디어를 명확하게 표현하기 위해 사용되는 표준 표기법들이다.

### 지수 함수 (Exponential Function):

* $y=e^x$: 오일러 수 $e$ (Euler's number, 약 2.71828)를 밑으로 하는 지수 함수이다. 연속적인 성장 또는 감쇠 현상을 모델링하는 데 기본이 된다.
* $y=\exp(x)$: $e^x$와 완전히 동일한 표현이다. 지수 부분이 복잡하거나 길 때, 예를 들어 $\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$처럼 표현하여 가독성을 높이기 위해 주로 사용된다.

### 로그 함수 (Logarithmic Function):

* $y=\ln(x)$ 또는 $y=\log_e(x)$: **자연로그(Natural logarithm)**로, 밑이 $e$인 로그이다. $e$를 몇 제곱해야 $x$가 되는지를 나타낸다 ($x>0$). 미적분학에서 특히 중요하며, 그 미분은 $\frac{1}{x}$로 간단하게 표현된다.
* $y=\log_{10}(x)$: **상용로그(Common logarithm)**로, 밑이 10인 로그이다. 10을 몇 제곱해야 $x$가 되는지를 나타낸다 ($x>0$). 과학 및 공학 분야에서 매우 큰 수나 작은 수의 크기 정도(order of magnitude)를 표현하거나, pH 척도, 데시벨(dB) 단위 등에서 사용된다.
* $y=\log_a(x)$: 밑이 $a$인 로그이다 ($a>0, a \neq 1, x>0$). $a$를 몇 제곱해야 $x$가 되는지를 나타낸다.
* $y=\log_2(x)$: **이진로그(Binary logarithm)**로, 밑이 2인 로그이다. 정보 이론(비트 수 계산)이나 컴퓨터 과학 분야에서 자주 사용된다.
* $y=\log(x)$: 문맥에 따라 의미가 달라질 수 있어 주의가 필요하다.
    * 수학, 특히 고등 수학 및 이론 분야에서는 대부분 $\ln(x)$ (자연로그)를 의미한다.
    * 컴퓨터 과학, 공학 또는 계산기에서는 $\log_{10}(x)$ (상용로그)나 때로는 $\log_2(x)$ (이진로그)를 의미하기도 한다.
## 2. 로그와 지수 함수의 역함수 관계

로그 함수와 지수 함수는 서로 역함수(Inverse function) 관계에 있다. 이는 하나의 함수 연산을 다른 함수가 "되돌리는" 효과를 가진다는 의미이다.

### 기본 정의로부터의 이해:

$\log_a(x)=y$ 라는 것은 " $a$를 $y$ 제곱하면 $x$가 된다" 즉, $a^y=x$ 와 동치(equivalent)이다.
이 관계를 통해 지수 표현을 로그 표현으로, 또는 그 반대로 변환할 수 있다.

### 역함수 관계식:

* $a^{\log_a(x)}=x$: 밑이 $a$인 로그를 취한 값($\log_a(x)$)을 다시 밑 $a$의 지수로 사용하면 원래 값 $x$가 된다. (단, $x>0$ 이어야 로그가 정의된다.)
* $\log_a(a^x)=x$: 밑이 $a$인 지수 함수 $a^x$에 다시 밑을 $a$로 하는 로그를 취하면 원래 지수 $x$가 된다. (모든 실수 $x$에 대해 성립한다.)

특히 자연로그($\ln$)와 자연 지수 함수($e^x$) 사이의 역함수 관계는 매우 중요하며 자주 사용된다:

* $e^{\ln(x)}=x$  (단, $x>0$)
* $\ln(e^x)=x$  (모든 실수 $x$에 대해 성립한다.)

이러한 역함수 관계는 로그 방정식을 풀거나 복잡한 수식을 단순화하는 데 유용하게 활용된다.

## 3. 시그모이드 함수 (Sigmoid Function)

시그모이드 함수는 이름에서 알 수 있듯이 S자 형태의 곡선을 가지는 함수들을 통칭한다. 그중 가장 대표적인 예가 바로 아래에서 다룰 로지스틱 함수이다.

### 대표적인 함수식 (로지스틱 함수 형태):

$f(x)=\frac{1}{1+e^{-kx}}$
($k$가 클수록 곡선 중앙 부분의 기울기가 가팔라진다.)

### 주요 특징:

* **출력 범위 (Output Range)**: 함수의 출력(치역)은 항상 $(0,1)$ 사이이다. 즉, $0 < f(x) < 1$ 이다. 이 때문에 어떤 값을 0과 1 사이의 확률이나 비율로 변환하는 데 매우 유용하다.
* **S-자 형태의 곡선 (S-curve)**: 입력 값 $x$가 음의 무한대($-\infty$)로 갈수록 $e^{-kx}$는 무한대로 커져 $f(x)$는 0에 수렴하고, $x$가 양의 무한대($+\infty$)로 갈수록 $e^{-kx}$는 0에 수렴하여 $f(x)$는 1에 수렴한다. 
* **중심점 (Center Point)**: 가장 기본적인 형태($x$축 평행이동이 없는 경우)에서 $x=0$일 때 $e^0=1$이므로, $f(0)=\frac{1}{1+1}=0.5$가 된다. 이 지점에서 곡선의 기울기가 최대가 되는 경우가 많다(로지스틱 함수의 경우).
* **단조 증가 (Monotonically Increasing)**: $k>0$일 때, $x$ 값이 증가하면 $f(x)$ 값도 항상 증가하거나 최소한 감소하지 않는다.
* **미분 가능 (Differentiable)**: 함수 전체 구간에서 미분이 가능하며, 그 도함수는 $f'(x) = k \cdot f(x)(1-f(x))$로 자기 자신을 이용해 간단히 표현된다. 이는 경사 하강법(gradient descent)과 같은 최적화 알고리즘에서 중요한 역할을 한다.
* **그래프**: 제시된 것처럼 $x$축 (예: $-6 \sim 6$)과 $y$축 ($0 \sim 1$) 범위에서 부드러운 S자 형태로 나타난다.


## 4. 로지스틱 함수 (Logistic Function) - 시그모이드 함수의 대표적인 예

로지스틱 함수는 시그모이드 함수의 한 종류로, 특히 로지스틱 회귀분석(Logistic Regression)에서 어떤 사건이 발생할 확률을 모델링하는 데 핵심적으로 사용된다. 이는 **로짓 변환(log-odds)**으로부터 자연스럽게 유도될 수 있다.

### 로짓 변환 (Logit Transformation) 이란?

어떤 사건이 발생할 확률을 $P$ ($0<P<1$)라고 할 때,

* **오즈 (Odds)**는 사건이 발생할 확률을 발생하지 않을 확률로 나눈 값이다: $\text{Odds}=\frac{P}{1-P}$. 오즈는 0부터 무한대까지의 값을 가진다.
* **로짓 (Logit)** 또는 **로그-오즈 (Log-odds)**는 이 오즈에 자연로그를 취한 값이다:
    $\text{logit}(P)=\ln\left(\frac{P}{1-P}\right)$

로짓 변환은 $(0,1)$ 범위의 확률 $P$를 $(-\infty, +\infty)$ 범위의 실수 전체 값으로 변환시켜 선형 모델을 적용하기 용이하게 만든다.

### 로짓 변환으로부터 로지스틱 함수 유도 과정:

로지스틱 회귀에서는 입력 변수들(features)의 선형 결합 $\beta_L = \beta_0 + \beta_1x_1 + \dots + \beta_nx_n$ 이 로짓 값과 같다고 가정한다.

$\ln\left(\frac{P}{1-P}\right) = \beta_L$

이제 이 식을 확률 $P$에 대해 정리한다:

양변에 지수 함수를 취함 (Exponentiate both sides):
$e^{\ln\left(\frac{P}{1-P}\right)} = e^{\beta_L}$
$\frac{P}{1-P} = e^{\beta_L}$

$P$에 대해 정리:
$P = e^{\beta_L}(1-P)$
$P = e^{\beta_L} - P \cdot e^{\beta_L}$
$P + P \cdot e^{\beta_L} = e^{\beta_L}$
$P(1+e^{\beta_L}) = e^{\beta_L}$

결론 (로지스틱 함수):
$P = \frac{e^{\beta_L}}{1+e^{\beta_L}}$

이 식의 분모와 분자를 $e^{\beta_L}$로 나누면 다음과 같은, 시그모이드 함수에서 본 익숙한 형태를 얻는다:
$P = \frac{e^{\beta_L}/e^{\beta_L}}{(1+e^{\beta_L})/e^{\beta_L}} = \frac{1}{e^{-\beta_L}+1} = \frac{1}{1+e^{-\beta_L}}$

여기서 $\beta_L$은 입력 특성들의 선형 결합 ($\beta_0 + \beta_1x_1 + \dots$)이다. 만약 $\beta_L = \beta x$ 이면, $P(x) = \frac{1}{1+e^{-\beta x}}$가 된다.

### 로지스틱 함수의 의미 및 활용:

* 입력값 $\beta_L$(독립 변수들의 선형 결합)을 0과 1 사이의 확률값 $P$로 변환한다.
* **로지스틱 회귀 (Logistic Regression)**: 이진 분류 문제(예: 스팸 메일 필터링 여부, 대출 상환 가능성, 질병 유무 판정)에서 특정 클래스에 속할 확률을 예측하는 데 핵심적인 역할을 한다. 모델은 입력 특성들의 선형 결합 $\beta_L$을 계산한 후, 이를 로지스틱 함수에 통과시켜 해당 사건이 발생할 확률을 얻는다.
* **결정 경계(Decision Boundary)**: 로지스틱 회귀에서 $P=0.5$가 되는 지점, 즉 $\beta_L=0$이 되는 지점을 기준으로 두 클래스를 분류한다.


## 5. 순위 변환(Rank Transform) 및 동점 처리

순위 변환은 데이터의 실제 값 대신 그 값의 순위를 사용하는 방법이다.

* **기본 순위 변환**: 데이터 값들을 크기순으로 정렬한 후, 순서대로 순위를 부여한다.
    * 예시: 데이터 집합 $S = \{4.001, 1, -10, 4, X_{\text{매우 큰 값}}\}$가 있을 때,
        * 값들을 오름차순으로 정렬하면: $\{-10, 1, 4, 4.001, X_{\text{매우 큰 값}}\}$
        * 이에 따른 순위는: $\{1, 2, 3, 4, 5\}$
        * 원래 $S$의 순서대로 각 요소의 순위를 표시한 순위 집합 $S_r = \{4, 2, 1, 3, 5\}$가 된다.
        (즉, $S$의 첫 번째 요소 $4.001$은 4순위, 두 번째 요소 $1$은 2순위 등)

* **동점(Ties) 처리**: 데이터에 동일한 값이 여러 개 있을 경우, 해당 값들이 받아야 할 순위들의 평균을 각 값의 순위로 할당한다.
    * 예시: 데이터 집합 $S = \{3, 1, -10, 4, 3\}$
        1.  값들을 오름차순으로 정렬: $\{-10, 1, 3, 3, 4\}$
        2.  정렬된 값들의 기본 순위: $\{1, 2, (\text{3과 4}), 5\}$
        3.  두 개의 $3$은 $3$번째와 $4$번째 순위를 차지하므로, 평균 순위는 $\frac{3+4}{2} = 3.5$가 된다.
        4.  원래 $S$의 순서대로 각 요소의 순위를 표시한 순위 집합 $S_r = \{3.5, 2, 1, 5, 3.5\}$가 된다.
        (즉, $S$의 첫 번째 요소 $3$은 $3.5$순위, 두 번째 요소 $1$은 $2$순위 등)
## 6. 추가 정리 및 주요점

* **용어의 관계**: "로지스틱 함수"는 "시그모이드 함수"의 가장 대표적이고 구체적인 한 예이다. 많은 문맥에서 두 용어는 거의 동일한 의미로 사용되기도 한다.
* **계수의 중요성 ($\beta_0, \beta_1, \dots$ 또는 $k$, $\beta_{coeff}$)**: 로지스틱 함수의 선형 결합 부분($\beta_L = \beta_0 + \beta_1x_1 + \dots$)에서의 계수들($\beta_1, \dots$) 또는 시그모이드/단순 로지스틱 형태($\frac{1}{1+e^{-kx}}$ 또는 $\frac{1}{1+e^{-\beta x}}$)에서의 계수($k$ 또는 $\beta$)는 해당 입력 변수가 결과 확률(의 로짓값)에 얼마나 큰 영향을 미치는지를 나타낸다. 또한 함수의 형태(가파르기)를 결정하여 모델이 입력 변화에 얼마나 민감하게 반응하는지를 조절한다.
* **다양한 변형**: 기본 로지스틱 함수 외에도, 최댓값($L$), 성장률($k$), 수평 이동($x_0$) 등을 조절하는 더 일반화된 형태의 로지스틱 함수 ($P(t)=\frac{L}{1+e^{-k(t-t_0)}}$)도 존재하며, 이는 인구 증가 모델(Verhulst model) 등 다양한 성장 현상을 설명하는 데 사용된다.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a161d9f5",
   "metadata": {},
   "source": [
    "## 1단계: TensorFlow의 기본 뼈대 이해하기\n",
    "\n",
    "- 하위 주제 1: 텐서플로우란? (핵심 원리: 그래프와 이식성)\n",
    "- 하위 주제 2: 텐서(Tensor)와 변수(Variable) (데이터의 기본 단위)\n",
    "- 하위 주제 3: 자동 미분과 GradientTape (학습의 엔진)\n",
    "\n",
    "## 2단계: 모델에 데이터 공급하기 (tf.data)\n",
    "\n",
    "- 하위 주제 1: 효율적인 데이터 파이프라인의 필요성\n",
    "- 하위 주제 2: tf.data.Dataset으로 데이터 불러오고 가공하기\n",
    "- 하위 주제 3: 성능 최적화의 비밀: prefetch\n",
    "\n",
    "## 3단계: 모델을 위한 재료 준비 (Feature Engineering)\n",
    "\n",
    "- 하위 주제 1: 특성(Feature)이란? (수치형 vs 범주형)\n",
    "- 하위 주제 2: 특성 열(Feature Column)의 마법 (원-핫 인코딩 & 임베딩)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f432e2d",
   "metadata": {},
   "source": [
    "\n",
    "### 1-1 TenserFlow?\n",
    "방향성 비순환 그래프(DAG, Directed Acyclic Graph)\n",
    "- Node: Operaion 에 해당\n",
    "- Edge: 데이터의 흐름 - Node 를 잇는 선\n",
    "\n",
    "**텐서(데이터)**가 그래프를 따라 흐르면서(Flow) 연산되는것\n",
    "\n",
    "파이썬으로 설계도를 생성하면 스마트폰, 웹브라우저, ... 다른 기기에서 Runtime이 실행하기 때문에 이식성(Portability)이 좋다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3611ddc",
   "metadata": {},
   "source": [
    "### 활동: 미니 레시피 분석하기\n",
    "TensorFlow로 `(3 + 5) * 2` 이라는 간단한 계산을 한다고 가정. \n",
    "\n",
    "1. 이 레시피에서 '재료를 손질하고 섞는' 것과 같은 실제 행동 단계(연산)에 해당하는 노드(Node)는 무엇일까요?\n",
    "2. 그 행동 단계들 사이를 흘러 다니는 '중간 재료(데이터)'에 해당하는 에지(Edge)는 무엇일까요?\n",
    "3. 이 레시피에 따라 요리를 마쳤을 때 나오는 최종 결과물, 16이라는 숫자는 TensorFlow에서 부르는 용어로 무엇일까요?\n",
    "\n",
    "- 노드 (Node): 덧셈, 곱셈 같은 연산.\n",
    "- 에지 (Edge): 연산과 연산을 잇는 데이터의 흐름/경로.\n",
    "- 텐서 (Tensor): 그 경로를 따라 흐르는 모든 데이터 (숫자, 벡터, 행렬 등).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187d6e5",
   "metadata": {},
   "source": [
    "\n",
    "### 1-2 Tensor와 Variable\n",
    "\n",
    "Tensor?\n",
    "데이터의 N차원 배열\n",
    "\n",
    "- Rank 0 Tensor: scalar - 단일 데이터\n",
    "- Rank 1 Tensor: Vector - 1차원 배열 Ex) - [1, 2, 3]\n",
    "- Rank 2 Tensor: matrix - 2차원 배열 Ex) - [[1, 2, 3], [4, 5, 6]]\n",
    "- Rank 3 Tensor: - 2차원 배열의 집합\n",
    "- Rank N Tensor: rank n - 1 텐서의 집합\n",
    "\n",
    "이전 텐서가 쌓여 이후 텐서를 구성한다.\n",
    "\n",
    "수학에서의 1, 2, 3`차원` 과는 다르게, `rank tensor`는 데이터에 접근하기 위한 인덱스의 갯수로 이해를 해야한다. \n",
    "\n",
    "- 벡터 -> data[i]\n",
    "- 행렬 -> data[i][j] -> 인덱스 2개\n",
    "- 3차원 텐서 -> data[i][j][k] -> 인덱스 3개\n",
    "\n",
    "#### 텐서의 종류\n",
    "\n",
    "1. Constant - 상수\n",
    "- tf.constant 로 생성. 이후 값은 불변\n",
    "\n",
    "2. Variable - 변수\n",
    "- tf.variable로 생성. 값을 변경 가능\n",
    "- **가중치(weights)**를 저장할때 사용\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac94fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상수 텐서: 10\n",
      "변수 텐서 (업데이트 전): 100.0\n",
      "변수 텐서 (업데이트 후): 110.0\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=110.0>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 상수 (Constant): 한 번 정해지면 바꿀 수 없어요.\n",
    "my_constant = tf.constant(10)\n",
    "print(f\"상수 텐서: {my_constant}\")\n",
    "\n",
    "# 2. 변수 (Variable): 값을 바꿀 수 있어요.\n",
    "my_variable = tf.Variable(100.0)\n",
    "print(f\"변수 텐서 (업데이트 전): {my_variable.numpy()}\")\n",
    "\n",
    "# 변수의 값을 10만큼 더해서 업데이트합니다. (학습 과정과 유사)\n",
    "my_variable.assign_add(10.0)\n",
    "print(f\"변수 텐서 (업데이트 후): {my_variable.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b37e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b1a9d",
   "metadata": {},
   "source": [
    "## 1-3 자동 미분과 GradientTape\n",
    "모델의 예측이 정답과 떨어진 정도를 나타내는 **손실(Loss)** 을 최소화 하는 방향으로 가중치를 조정해야 한다.\n",
    "\n",
    "**Loss** 가 가장 기울어 져 있는 **경사(Gradient)** 를 찾고, 가중치를 조정하는 과정을 **경사하강법(Gradient Descent)** 이라 한다.\n",
    "\n",
    "**Loss** 가 기울어진 정도는 Loss함수를 미분하여 구할 수 있지만, 모든 가중치에 대해 손으로 계산하는 것은 불가능\n",
    "\n",
    "`tf.GradientTape` 으로 효율적이고 쉽게 계산 가능하다.\n",
    "\n",
    "- 녹화 시작: with tf.GradientTape() as tape: 라는 코드로 녹화기를 킨다.\n",
    "- 연산 녹화: 녹화기가 켜진 동안, 모델이 예측값을 만들고 손실(loss)을 계산하는 모든 과정을 테이프에 차곡차곡 기록한다. \n",
    "- 테이프 되감기 (미분): 녹화가 끝나면, tape.gradient(loss, weights) 라는 명령으로 테이프를 거꾸로 되감는다. 이 과정에서 TensorFlow는 **연쇄 법칙(Chain Rule)**을  이용해 손실 값에 영향을 미친 모든 가중치의 그래디언트를 자동으로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2174be67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 손실 값: 16.0\n",
      "가중치 W에 대한 그래디언트: -16.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 모델의 가중치(Variable)와 입력값, 정답을 준비합니다.\n",
    "W = tf.Variable(3.0) # 현재 가중치\n",
    "x = 2.0              # 입력 데이터\n",
    "y_true = 10.0        # 실제 정답\n",
    "\n",
    "# 1. 녹화 시작!\n",
    "with tf.GradientTape() as tape:\n",
    "  # 2. 녹화 중에 예측하고 손실을 계산합니다.\n",
    "  y_pred = W * x   # 예측: 3.0 * 2.0 = 6.0\n",
    "  loss = (y_pred - y_true)**2  # 손실: (6.0 - 10.0)^2 = 16.0\n",
    "\n",
    "# 3. 녹화된 테이프를 이용해 그래디언트를 계산합니다.\n",
    "# \"loss(16.0)를 줄이려면 W(3.0)를 어느 방향으로 바꿔야 할까?\"\n",
    "grad_W = tape.gradient(loss, W)\n",
    "\n",
    "print(f\"현재 손실 값: {loss.numpy()}\")\n",
    "print(f\"가중치 W에 대한 그래디언트: {grad_W.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34045083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
